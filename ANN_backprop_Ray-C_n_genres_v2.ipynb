{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPNN CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100,activation_o = f_softmax):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                        activation=f_sigmoid))\n",
    "\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=activation_o))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        #The differnce between the predicted labels from the neural network and the correct labels\n",
    "        #Calculates the derivate of the cost function\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        \n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "\n",
    "            #Backpropagates the delta with respect to the derivate of the activation function\n",
    "            #Bakcpropagates the partial derivates\n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            #Delta is used to know how the weights should be adjusted to minimize the error\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=170, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #It updates the weighs by multiplying the error with the output from itself and the delta \n",
    "                #from the next node in the network\n",
    "                #eta is how aggressive the network \"corrects itself\" to changes\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    print((labels.shape[0], nbits))\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    print(bit_vector)\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels, batched_valid_data, batched_valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_to_bit_vector(genres, nbits):\n",
    "    bit_vector = np.zeros((genres.shape[0], nbits))\n",
    "    nr_genres = 0\n",
    "    i =  0\n",
    "    while nr_genres<genres.shape[0]:\n",
    "        try:\n",
    "            if(mydict[genres[i]]!= 0 | mydict[genres[i]]!= 4):\n",
    "                bit_vector[nr_genres, 2] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "            else:\n",
    "                bit_vector[nr_genres, mydict[genres[i]]] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "        except KeyError:\n",
    "            i += 1\n",
    "    \n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input genres to a dictionary\n",
    "def createDict(input_genres):\n",
    "    mydict={}\n",
    "    i = 0\n",
    "    for item in input_genres:\n",
    "        if(i>0 and item in mydict):\n",
    "            continue\n",
    "        else:    \n",
    "           mydict[item] = i\n",
    "           i = i+1\n",
    "    return mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print(\"This is : \" + str(N))\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = genre_to_bit_vector(labels[idx:idx+batch_size], len(input_genres))\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "    return chunked_data, chunked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniset(input_genres):\n",
    "    # Load metadata from FMA\n",
    "    tracks = utils.load('./fma_metadata/tracks.csv')\n",
    "    genres = utils.load('./fma_metadata/genres.csv')\n",
    "    features = utils.load('./fma_metadata/features.csv')\n",
    "    echonest = utils.load('./fma_metadata/echonest.csv')\n",
    "\n",
    "    np.testing.assert_array_equal(features.index, tracks.index)\n",
    "    assert echonest.index.isin(tracks.index).all()\n",
    "    \n",
    "    # Split the set of tracks into partitions for training and test\n",
    "    small = tracks['set', 'subset'] <= 'small'\n",
    "    train = tracks['set', 'split'] == 'training'\n",
    "    test = (tracks['set', 'split'] == 'test') | (tracks['set', 'split'] == 'validation')\n",
    "\n",
    "    # Take out track_id for specified genres to create smaller subset\n",
    "    genres = tracks.loc[small, ('track', 'genre_top')]\n",
    "    mini_set = genres[genres.isin(input_genres)]\n",
    "    \n",
    "    # Create sets for training and testing\n",
    "    X_test = features.loc[small & test, 'mfcc']\n",
    "    X_test = X_test.values\n",
    "\n",
    "    Xtr = features.loc[small & train, 'mfcc']\n",
    "    Xtr = Xtr.values\n",
    "    \n",
    "    L_test = tracks.loc[small & test, ('track', 'genre_top')]\n",
    "    Ltr = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "\n",
    "    # Take out the tracks from the specified genres\n",
    "    Ltr_mini = Ltr[Ltr.isin(input_genres)]\n",
    "    Xtr_mini = []\n",
    "    i = 0\n",
    "    for j in Ltr:\n",
    "        if(j in input_genres):\n",
    "            Xtr_mini.append(Xtr[i])\n",
    "        i += 1 \n",
    "    Xtr_mini = np.asarray(Xtr_mini)\n",
    "\n",
    "    L_test_mini = L_test[L_test.isin(input_genres)]\n",
    "    X_test_mini = []\n",
    "    i = 0\n",
    "    for j in L_test:\n",
    "        if((j in input_genres)):\n",
    "            X_test_mini.append(X_test[i])\n",
    "        i += 1 \n",
    "    X_test_mini = np.asarray(X_test_mini)\n",
    "    \n",
    "    # Normalize data\n",
    "    mean = np.mean(Xtr_mini, axis=0)\n",
    "    std = np.std(Xtr_mini, axis=0)\n",
    "\n",
    "    Xtr_mini = (Xtr_mini - mean)/std\n",
    "    X_test_mini = (X_test_mini - mean)/std\n",
    "    \n",
    "    return Xtr_mini, Ltr_mini, X_test_mini, L_test_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_genres):\n",
    "    mydict = createDict(input_genres)\n",
    "    Xtr_mini, Ltr_mini, X_test_mini, L_test_mini = miniset(input_genres)\n",
    "    return mydict, Xtr_mini, Ltr_mini, X_test_mini, L_test_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose which genres to compare\n",
    "Select between the following genres: <br>\n",
    "['Hip-Hop' ,'Rock' ,'Pop' ,'Folk' ,'Experimental' ,'International' ,'Electronic' ,'Instrumental'] <br>\n",
    "Set batch size to <= #genres * 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "This is : 3200\n",
      "Batch size 800, the number of examples 3200.\n",
      "This is : 800\n",
      "Batch size 800, the number of examples 800.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "input_genres = ['Rock','Pop','Folk','Electronic']\n",
    "batch_size = 800\n",
    "# Create training and test sets\n",
    "mydict, Xtr_mini, Ltr_mini, X_test_mini, L_test_mini = preprocess(input_genres)\n",
    "# Create batches\n",
    "train_data, train_labels, valid_data, valid_labels = prepare_for_backprop(batch_size, Xtr_mini, Ltr_mini, X_test_mini, L_test_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing input layer with size 140.\n",
      "Initializing hidden layer with size 20.\n",
      "Initializing output layer with size 4.\n",
      "Done!\n",
      "Training for 170 epochs...\n",
      "[   0]  Training error: 0.75000 Test error: 0.75000\n",
      "[   1]  Training error: 0.75000 Test error: 0.75000\n",
      "[   2]  Training error: 0.75000 Test error: 0.75000\n",
      "[   3]  Training error: 0.75000 Test error: 0.75000\n",
      "[   4]  Training error: 0.64125 Test error: 0.64500\n",
      "[   5]  Training error: 0.73594 Test error: 0.72500\n",
      "[   6]  Training error: 0.71594 Test error: 0.70125\n",
      "[   7]  Training error: 0.58156 Test error: 0.55000\n",
      "[   8]  Training error: 0.48656 Test error: 0.47000\n",
      "[   9]  Training error: 0.45375 Test error: 0.41000\n",
      "[  10]  Training error: 0.42063 Test error: 0.38000\n",
      "[  11]  Training error: 0.39500 Test error: 0.37875\n",
      "[  12]  Training error: 0.38719 Test error: 0.37625\n",
      "[  13]  Training error: 0.38250 Test error: 0.37125\n",
      "[  14]  Training error: 0.37656 Test error: 0.37000\n",
      "[  15]  Training error: 0.36938 Test error: 0.36375\n",
      "[  16]  Training error: 0.36406 Test error: 0.36375\n",
      "[  17]  Training error: 0.35406 Test error: 0.37125\n",
      "[  18]  Training error: 0.34563 Test error: 0.36875\n",
      "[  19]  Training error: 0.34219 Test error: 0.36875\n",
      "[  20]  Training error: 0.33781 Test error: 0.36750\n",
      "[  21]  Training error: 0.33344 Test error: 0.36875\n",
      "[  22]  Training error: 0.33125 Test error: 0.36250\n",
      "[  23]  Training error: 0.32500 Test error: 0.36625\n",
      "[  24]  Training error: 0.31875 Test error: 0.35875\n",
      "[  25]  Training error: 0.31594 Test error: 0.35625\n",
      "[  26]  Training error: 0.31438 Test error: 0.35500\n",
      "[  27]  Training error: 0.31281 Test error: 0.35875\n",
      "[  28]  Training error: 0.31125 Test error: 0.35000\n",
      "[  29]  Training error: 0.30594 Test error: 0.35000\n",
      "[  30]  Training error: 0.30188 Test error: 0.35125\n",
      "[  31]  Training error: 0.30000 Test error: 0.35375\n",
      "[  32]  Training error: 0.29625 Test error: 0.35500\n",
      "[  33]  Training error: 0.29500 Test error: 0.35375\n",
      "[  34]  Training error: 0.29406 Test error: 0.35375\n",
      "[  35]  Training error: 0.29344 Test error: 0.35750\n",
      "[  36]  Training error: 0.29125 Test error: 0.35500\n",
      "[  37]  Training error: 0.28937 Test error: 0.35625\n",
      "[  38]  Training error: 0.28687 Test error: 0.35875\n",
      "[  39]  Training error: 0.28500 Test error: 0.35500\n",
      "[  40]  Training error: 0.28469 Test error: 0.35250\n",
      "[  41]  Training error: 0.28531 Test error: 0.35250\n",
      "[  42]  Training error: 0.28437 Test error: 0.35375\n",
      "[  43]  Training error: 0.28344 Test error: 0.35375\n",
      "[  44]  Training error: 0.28125 Test error: 0.35250\n",
      "[  45]  Training error: 0.27906 Test error: 0.35250\n",
      "[  46]  Training error: 0.27937 Test error: 0.35375\n",
      "[  47]  Training error: 0.27875 Test error: 0.35750\n",
      "[  48]  Training error: 0.27875 Test error: 0.35875\n",
      "[  49]  Training error: 0.27813 Test error: 0.36125\n",
      "[  50]  Training error: 0.27813 Test error: 0.36250\n",
      "[  51]  Training error: 0.27781 Test error: 0.36250\n",
      "[  52]  Training error: 0.27937 Test error: 0.36500\n",
      "[  53]  Training error: 0.27875 Test error: 0.36250\n",
      "[  54]  Training error: 0.27813 Test error: 0.36375\n",
      "[  55]  Training error: 0.27781 Test error: 0.36625\n",
      "[  56]  Training error: 0.27656 Test error: 0.37125\n",
      "[  57]  Training error: 0.27563 Test error: 0.37250\n",
      "[  58]  Training error: 0.27500 Test error: 0.37625\n",
      "[  59]  Training error: 0.27437 Test error: 0.37625\n",
      "[  60]  Training error: 0.27344 Test error: 0.37750\n",
      "[  61]  Training error: 0.27344 Test error: 0.37625\n",
      "[  62]  Training error: 0.27313 Test error: 0.37750\n",
      "[  63]  Training error: 0.27344 Test error: 0.37625\n",
      "[  64]  Training error: 0.27219 Test error: 0.37625\n",
      "[  65]  Training error: 0.27156 Test error: 0.37625\n",
      "[  66]  Training error: 0.27094 Test error: 0.37625\n",
      "[  67]  Training error: 0.26969 Test error: 0.37500\n",
      "[  68]  Training error: 0.26969 Test error: 0.37625\n",
      "[  69]  Training error: 0.26875 Test error: 0.37625\n",
      "[  70]  Training error: 0.26906 Test error: 0.37750\n",
      "[  71]  Training error: 0.26750 Test error: 0.37875\n",
      "[  72]  Training error: 0.26719 Test error: 0.37875\n",
      "[  73]  Training error: 0.26656 Test error: 0.37875\n",
      "[  74]  Training error: 0.26594 Test error: 0.37750\n",
      "[  75]  Training error: 0.26531 Test error: 0.37750\n",
      "[  76]  Training error: 0.26594 Test error: 0.37750\n",
      "[  77]  Training error: 0.26562 Test error: 0.37625\n",
      "[  78]  Training error: 0.26469 Test error: 0.37500\n",
      "[  79]  Training error: 0.26281 Test error: 0.37500\n",
      "[  80]  Training error: 0.26312 Test error: 0.37625\n",
      "[  81]  Training error: 0.26344 Test error: 0.37750\n",
      "[  82]  Training error: 0.26250 Test error: 0.37750\n",
      "[  83]  Training error: 0.26062 Test error: 0.37625\n",
      "[  84]  Training error: 0.26094 Test error: 0.37750\n",
      "[  85]  Training error: 0.26062 Test error: 0.37625\n",
      "[  86]  Training error: 0.25969 Test error: 0.37625\n",
      "[  87]  Training error: 0.25969 Test error: 0.37625\n",
      "[  88]  Training error: 0.25938 Test error: 0.37625\n",
      "[  89]  Training error: 0.25875 Test error: 0.37625\n",
      "[  90]  Training error: 0.25688 Test error: 0.37625\n",
      "[  91]  Training error: 0.25625 Test error: 0.37750\n",
      "[  92]  Training error: 0.25594 Test error: 0.37750\n",
      "[  93]  Training error: 0.25500 Test error: 0.37750\n",
      "[  94]  Training error: 0.25469 Test error: 0.37750\n",
      "[  95]  Training error: 0.25500 Test error: 0.37750\n",
      "[  96]  Training error: 0.25375 Test error: 0.37750\n",
      "[  97]  Training error: 0.25312 Test error: 0.37750\n",
      "[  98]  Training error: 0.25344 Test error: 0.37750\n",
      "[  99]  Training error: 0.25312 Test error: 0.37750\n",
      "[ 100]  Training error: 0.25281 Test error: 0.37500\n",
      "[ 101]  Training error: 0.25312 Test error: 0.37500\n",
      "[ 102]  Training error: 0.25312 Test error: 0.37375\n",
      "[ 103]  Training error: 0.25250 Test error: 0.37250\n",
      "[ 104]  Training error: 0.25156 Test error: 0.37250\n",
      "[ 105]  Training error: 0.25188 Test error: 0.37250\n",
      "[ 106]  Training error: 0.25094 Test error: 0.37500\n",
      "[ 107]  Training error: 0.25031 Test error: 0.37500\n",
      "[ 108]  Training error: 0.25031 Test error: 0.37375\n",
      "[ 109]  Training error: 0.24938 Test error: 0.37500\n",
      "[ 110]  Training error: 0.24906 Test error: 0.37500\n",
      "[ 111]  Training error: 0.24813 Test error: 0.37500\n",
      "[ 112]  Training error: 0.24781 Test error: 0.37375\n",
      "[ 113]  Training error: 0.24750 Test error: 0.37500\n",
      "[ 114]  Training error: 0.24719 Test error: 0.37375\n",
      "[ 115]  Training error: 0.24688 Test error: 0.37375\n",
      "[ 116]  Training error: 0.24625 Test error: 0.37250\n",
      "[ 117]  Training error: 0.24563 Test error: 0.37250\n",
      "[ 118]  Training error: 0.24438 Test error: 0.37250\n",
      "[ 119]  Training error: 0.24375 Test error: 0.37500\n",
      "[ 120]  Training error: 0.24281 Test error: 0.37500\n",
      "[ 121]  Training error: 0.24188 Test error: 0.37625\n",
      "[ 122]  Training error: 0.24125 Test error: 0.37625\n",
      "[ 123]  Training error: 0.24063 Test error: 0.37750\n",
      "[ 124]  Training error: 0.24063 Test error: 0.37750\n",
      "[ 125]  Training error: 0.24031 Test error: 0.37750\n",
      "[ 126]  Training error: 0.24031 Test error: 0.37750\n",
      "[ 127]  Training error: 0.24000 Test error: 0.37750\n",
      "[ 128]  Training error: 0.23969 Test error: 0.37750\n",
      "[ 129]  Training error: 0.23875 Test error: 0.37750\n",
      "[ 130]  Training error: 0.23781 Test error: 0.37875\n",
      "[ 131]  Training error: 0.23719 Test error: 0.37875\n",
      "[ 132]  Training error: 0.23563 Test error: 0.38125\n",
      "[ 133]  Training error: 0.23469 Test error: 0.38375\n",
      "[ 134]  Training error: 0.23438 Test error: 0.38250\n",
      "[ 135]  Training error: 0.23219 Test error: 0.38125\n",
      "[ 136]  Training error: 0.23219 Test error: 0.38125\n",
      "[ 137]  Training error: 0.23156 Test error: 0.38375\n",
      "[ 138]  Training error: 0.23156 Test error: 0.38375\n",
      "[ 139]  Training error: 0.23094 Test error: 0.38375\n",
      "[ 140]  Training error: 0.22906 Test error: 0.38500\n",
      "[ 141]  Training error: 0.22906 Test error: 0.38500\n",
      "[ 142]  Training error: 0.22875 Test error: 0.38500\n",
      "[ 143]  Training error: 0.22687 Test error: 0.38375\n",
      "[ 144]  Training error: 0.22437 Test error: 0.38375\n",
      "[ 145]  Training error: 0.22375 Test error: 0.38375\n",
      "[ 146]  Training error: 0.22312 Test error: 0.38500\n",
      "[ 147]  Training error: 0.22250 Test error: 0.38500\n",
      "[ 148]  Training error: 0.22156 Test error: 0.38500\n",
      "[ 149]  Training error: 0.22094 Test error: 0.38500\n",
      "[ 150]  Training error: 0.22062 Test error: 0.38500\n",
      "[ 151]  Training error: 0.22000 Test error: 0.38375\n",
      "[ 152]  Training error: 0.21875 Test error: 0.38375\n",
      "[ 153]  Training error: 0.21875 Test error: 0.38500\n",
      "[ 154]  Training error: 0.21844 Test error: 0.38500\n",
      "[ 155]  Training error: 0.21781 Test error: 0.38500\n",
      "[ 156]  Training error: 0.21719 Test error: 0.38250\n",
      "[ 157]  Training error: 0.21688 Test error: 0.38250\n",
      "[ 158]  Training error: 0.21656 Test error: 0.38250\n",
      "[ 159]  Training error: 0.21594 Test error: 0.38500\n",
      "[ 160]  Training error: 0.21563 Test error: 0.38500\n",
      "[ 161]  Training error: 0.21563 Test error: 0.38750\n",
      "[ 162]  Training error: 0.21531 Test error: 0.38750\n",
      "[ 163]  Training error: 0.21563 Test error: 0.38750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 164]  Training error: 0.21406 Test error: 0.38875\n",
      "[ 165]  Training error: 0.21438 Test error: 0.38875\n",
      "[ 166]  Training error: 0.21281 Test error: 0.38875\n",
      "[ 167]  Training error: 0.21281 Test error: 0.38875\n",
      "[ 168]  Training error: 0.21250 Test error: 0.39000\n",
      "[ 169]  Training error: 0.21250 Test error: 0.38875\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(layer_config=[140,20, len(input_genres)], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eta=0.001,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
