{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPNN CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "\n",
    "#def f_relu(X):\n",
    " #   return np.maximum(0,X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print(sys.stderr, err_str)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100,activation_o = f_softmax):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                        activation=f_sigmoid))\n",
    "\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=activation_o))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #Check!\n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        \n",
    "        #The differnce between the predicted labels from the neural network and the correct labels\n",
    "        #Calculates the derivate of the cost function\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        \n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "           #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            #Backpropagates the delta with respect to the derivate of the activation function\n",
    "            #Bakcpropagates the partial derivates\n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            #Delta is used to know how the weights should be adjusted to minimize the error\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=170, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                \n",
    "                #It updates the weighs by multiplying the error with the output from itself and the delta \n",
    "                #from the next node in the network\n",
    "                #eta is how aggressive the network \"corrects itself\" to changes\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    print((labels.shape[0], nbits))\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    print(bit_vector)\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_to_bit_vector(genres, nbits):\n",
    "        \n",
    "    #print((genres.shape[0], nbits))\n",
    "    bit_vector = np.zeros((genres.shape[0], nbits))\n",
    "    \n",
    "    #print(bit_vector)\n",
    "    nr_genres = 0\n",
    "    i =  0\n",
    "    while nr_genres<genres.shape[0]:\n",
    "        try:\n",
    "            #print(str(nr_genres) + \" \" + str(genres[i]) )\n",
    "            if(mydict[genres[i]]!= 0 | mydict[genres[i]]!= 4):\n",
    "                bit_vector[nr_genres, 2] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "            else:\n",
    "                #print(mydict[genres[i]])\n",
    "                bit_vector[nr_genres, mydict[genres[i]]] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "        except KeyError:\n",
    "            i += 1\n",
    "    \n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram(songID,resample=False ,sampling=44100, plot = True):\n",
    "    filepath = utils.get_audio_path('./fma_small/', songID)\n",
    "    try:\n",
    "        x, sr = librosa.load(filepath, sr=None, mono=True)  # kaiser_fast\n",
    "        print(sr)\n",
    "    except FileNotFoundError:\n",
    "        return -1\n",
    "        \n",
    "    \n",
    "    if(resample):\n",
    "        x = librosa.resample(x, sr, sampling)\n",
    "        sr = sampling\n",
    "    spec = librosa.feature.melspectrogram(y=x, sr=sr)\n",
    "    \n",
    "    if(plot):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis='mel', fmax=sr, x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel spectrogram')\n",
    "        plt.tight_layout()\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['Hip-Hop','Rock','Pop','Folk','Experimental','International','Electronic','Instrumental']\n",
    "input_genres = ['Rock','Pop','Folk','Experimental']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rock': 0, 'Pop': 1, 'Folk': 2, 'Experimental': 3}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict={}\n",
    "i = 0\n",
    "for item in input_genres:\n",
    "    if(i>0 and item in mydict):\n",
    "        continue\n",
    "    else:    \n",
    "       mydict[item] = i\n",
    "       i = i+1\n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/anaconda3/envs/D7041E/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tracks = utils.load('./fma_metadata/tracks.csv')\n",
    "genres = utils.load('./fma_metadata/genres.csv')\n",
    "features = utils.load('./fma_metadata/features.csv')\n",
    "echonest = utils.load('./fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print(\"This is : \" + str(N))\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    #print(data[:10])\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = genre_to_bit_vector(labels[idx:idx+batch_size], len(input_genres))\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = tracks['set', 'subset'] <= 'small'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "val = tracks['set', 'split'] == 'validation'\n",
    "test = tracks['set', 'split'] == 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = tracks.loc[small, ('track', 'genre_top')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = tracks.loc[small, ('track', 'genre_top')]\n",
    "# Take out track_id for specified genres to create smaller subset\n",
    "mini_set = genres[genres.isin(input_genres)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = features.loc[small & test, 'mfcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = features.loc[small & train, 'mfcc']\n",
    "Xtr = Xtr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ltr = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "#Ltr = Ltr[:3200] \n",
    "\n",
    "# Take out track_id for specified genres to create smaller subset\n",
    "Ltr_mini = Ltr[Ltr.isin(input_genres)]\n",
    "Xtr_mini = []\n",
    "i = 0\n",
    "for j in Ltr:\n",
    "    if(j in input_genres):\n",
    "        Xtr_mini.append(Xtr[i])\n",
    "    i += 1 \n",
    "Xtr_mini = np.asarray(Xtr_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_test = tracks.loc[small & test, ('track', 'genre_top')]\n",
    "\n",
    "# Take out track_id for specified genres to create smaller subset\n",
    "L_test_mini = L_test[L_test.isin(input_genres)]\n",
    "X_test_mini = []\n",
    "i = 0\n",
    "for j in L_test:\n",
    "    if((j in input_genres)):\n",
    "        X_test_mini.append(X_test[i])\n",
    "    i += 1 \n",
    "X_test_mini = np.asarray(X_test_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "400\n",
      "3200\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_mini))\n",
    "print(len(L_test_mini))\n",
    "print(len(Xtr_mini))\n",
    "#print(L_test_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(Xtr_mini, axis=0)\n",
    "std = np.std(Xtr_mini, axis=0)\n",
    "\n",
    "Xtr_mini = (Xtr_mini - mean)/std\n",
    "X_test_mini = (X_test_mini - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "This is : 3200\n",
      "Batch size 400, the number of examples 3200.\n",
      "This is : 400\n",
      "Batch size 400, the number of examples 400.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "batch_size=400;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr_mini, Ltr_mini, X_test_mini, L_test_mini)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default activation eta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing input layer with size 140.\n",
      "Initializing hidden layer with size 20.\n",
      "Initializing output layer with size 4.\n",
      "Done!\n",
      "Training for 170 epochs...\n",
      "[   0]  Training error: 0.75000 Test error: 0.75000\n",
      "[   1]  Training error: 0.75000 Test error: 0.75000\n",
      "[   2]  Training error: 0.75000 Test error: 0.75000\n",
      "[   3]  Training error: 0.75000 Test error: 0.75000\n",
      "[   4]  Training error: 0.75000 Test error: 0.75000\n",
      "[   5]  Training error: 0.60750 Test error: 0.60000\n",
      "[   6]  Training error: 0.63469 Test error: 0.61750\n",
      "[   7]  Training error: 0.60500 Test error: 0.59750\n",
      "[   8]  Training error: 0.58562 Test error: 0.59750\n",
      "[   9]  Training error: 0.57031 Test error: 0.59750\n",
      "[  10]  Training error: 0.55781 Test error: 0.58750\n",
      "[  11]  Training error: 0.54156 Test error: 0.56750\n",
      "[  12]  Training error: 0.53031 Test error: 0.54750\n",
      "[  13]  Training error: 0.51687 Test error: 0.55000\n",
      "[  14]  Training error: 0.50875 Test error: 0.53750\n",
      "[  15]  Training error: 0.49750 Test error: 0.54000\n",
      "[  16]  Training error: 0.49469 Test error: 0.53000\n",
      "[  17]  Training error: 0.48906 Test error: 0.53750\n",
      "[  18]  Training error: 0.47656 Test error: 0.53250\n",
      "[  19]  Training error: 0.45156 Test error: 0.52500\n",
      "[  20]  Training error: 0.43125 Test error: 0.49500\n",
      "[  21]  Training error: 0.41906 Test error: 0.47750\n",
      "[  22]  Training error: 0.40844 Test error: 0.48250\n",
      "[  23]  Training error: 0.40125 Test error: 0.47500\n",
      "[  24]  Training error: 0.39437 Test error: 0.47500\n",
      "[  25]  Training error: 0.38969 Test error: 0.47750\n",
      "[  26]  Training error: 0.38594 Test error: 0.47000\n",
      "[  27]  Training error: 0.38312 Test error: 0.47500\n",
      "[  28]  Training error: 0.37812 Test error: 0.47500\n",
      "[  29]  Training error: 0.37750 Test error: 0.46750\n",
      "[  30]  Training error: 0.37375 Test error: 0.46500\n",
      "[  31]  Training error: 0.36906 Test error: 0.46250\n",
      "[  32]  Training error: 0.36562 Test error: 0.45750\n",
      "[  33]  Training error: 0.36156 Test error: 0.45500\n",
      "[  34]  Training error: 0.35781 Test error: 0.45500\n",
      "[  35]  Training error: 0.35500 Test error: 0.45750\n",
      "[  36]  Training error: 0.35313 Test error: 0.45500\n",
      "[  37]  Training error: 0.35250 Test error: 0.45250\n",
      "[  38]  Training error: 0.35406 Test error: 0.45250\n",
      "[  39]  Training error: 0.35187 Test error: 0.45750\n",
      "[  40]  Training error: 0.34969 Test error: 0.46000\n",
      "[  41]  Training error: 0.34906 Test error: 0.46500\n",
      "[  42]  Training error: 0.34906 Test error: 0.46750\n",
      "[  43]  Training error: 0.34781 Test error: 0.46750\n",
      "[  44]  Training error: 0.34844 Test error: 0.47000\n",
      "[  45]  Training error: 0.34719 Test error: 0.47250\n",
      "[  46]  Training error: 0.34687 Test error: 0.47750\n",
      "[  47]  Training error: 0.34500 Test error: 0.47750\n",
      "[  48]  Training error: 0.34281 Test error: 0.47750\n",
      "[  49]  Training error: 0.34281 Test error: 0.48250\n",
      "[  50]  Training error: 0.34031 Test error: 0.48500\n",
      "[  51]  Training error: 0.34031 Test error: 0.48750\n",
      "[  52]  Training error: 0.34063 Test error: 0.49000\n",
      "[  53]  Training error: 0.33937 Test error: 0.49250\n",
      "[  54]  Training error: 0.33937 Test error: 0.49000\n",
      "[  55]  Training error: 0.33969 Test error: 0.49500\n",
      "[  56]  Training error: 0.33969 Test error: 0.49250\n",
      "[  57]  Training error: 0.33875 Test error: 0.49750\n",
      "[  58]  Training error: 0.33781 Test error: 0.49750\n",
      "[  59]  Training error: 0.33719 Test error: 0.50000\n",
      "[  60]  Training error: 0.33625 Test error: 0.50250\n",
      "[  61]  Training error: 0.33563 Test error: 0.50500\n",
      "[  62]  Training error: 0.33563 Test error: 0.50000\n",
      "[  63]  Training error: 0.33469 Test error: 0.49500\n",
      "[  64]  Training error: 0.33344 Test error: 0.49500\n",
      "[  65]  Training error: 0.33219 Test error: 0.49500\n",
      "[  66]  Training error: 0.33219 Test error: 0.49500\n",
      "[  67]  Training error: 0.33031 Test error: 0.49500\n",
      "[  68]  Training error: 0.33000 Test error: 0.49750\n",
      "[  69]  Training error: 0.33000 Test error: 0.50250\n",
      "[  70]  Training error: 0.32969 Test error: 0.50250\n",
      "[  71]  Training error: 0.32969 Test error: 0.50250\n",
      "[  72]  Training error: 0.32906 Test error: 0.50250\n",
      "[  73]  Training error: 0.32750 Test error: 0.50250\n",
      "[  74]  Training error: 0.32500 Test error: 0.50500\n",
      "[  75]  Training error: 0.32438 Test error: 0.50500\n",
      "[  76]  Training error: 0.32469 Test error: 0.50750\n",
      "[  77]  Training error: 0.32312 Test error: 0.51000\n",
      "[  78]  Training error: 0.32312 Test error: 0.51250\n",
      "[  79]  Training error: 0.32250 Test error: 0.51250\n",
      "[  80]  Training error: 0.32188 Test error: 0.51250\n",
      "[  81]  Training error: 0.32188 Test error: 0.51250\n",
      "[  82]  Training error: 0.32156 Test error: 0.51250\n",
      "[  83]  Training error: 0.32125 Test error: 0.51000\n",
      "[  84]  Training error: 0.32125 Test error: 0.51000\n",
      "[  85]  Training error: 0.32062 Test error: 0.51250\n",
      "[  86]  Training error: 0.32000 Test error: 0.51250\n",
      "[  87]  Training error: 0.31906 Test error: 0.51250\n",
      "[  88]  Training error: 0.31812 Test error: 0.51250\n",
      "[  89]  Training error: 0.31719 Test error: 0.51250\n",
      "[  90]  Training error: 0.31719 Test error: 0.51250\n",
      "[  91]  Training error: 0.31500 Test error: 0.51250\n",
      "[  92]  Training error: 0.31438 Test error: 0.51250\n",
      "[  93]  Training error: 0.31469 Test error: 0.51500\n",
      "[  94]  Training error: 0.31375 Test error: 0.51500\n",
      "[  95]  Training error: 0.31250 Test error: 0.51500\n",
      "[  96]  Training error: 0.31219 Test error: 0.51250\n",
      "[  97]  Training error: 0.31094 Test error: 0.51250\n",
      "[  98]  Training error: 0.31062 Test error: 0.51500\n",
      "[  99]  Training error: 0.31031 Test error: 0.51250\n",
      "[ 100]  Training error: 0.30938 Test error: 0.51250\n",
      "[ 101]  Training error: 0.30938 Test error: 0.51000\n",
      "[ 102]  Training error: 0.30906 Test error: 0.50750\n",
      "[ 103]  Training error: 0.30969 Test error: 0.50750\n",
      "[ 104]  Training error: 0.30969 Test error: 0.50750\n",
      "[ 105]  Training error: 0.30781 Test error: 0.50500\n",
      "[ 106]  Training error: 0.30688 Test error: 0.50250\n",
      "[ 107]  Training error: 0.30688 Test error: 0.50500\n",
      "[ 108]  Training error: 0.30625 Test error: 0.50000\n",
      "[ 109]  Training error: 0.30656 Test error: 0.50000\n",
      "[ 110]  Training error: 0.30531 Test error: 0.50000\n",
      "[ 111]  Training error: 0.30469 Test error: 0.50000\n",
      "[ 112]  Training error: 0.30438 Test error: 0.50000\n",
      "[ 113]  Training error: 0.30344 Test error: 0.50000\n",
      "[ 114]  Training error: 0.30250 Test error: 0.50000\n",
      "[ 115]  Training error: 0.30125 Test error: 0.50250\n",
      "[ 116]  Training error: 0.30125 Test error: 0.50000\n",
      "[ 117]  Training error: 0.29969 Test error: 0.49750\n",
      "[ 118]  Training error: 0.29906 Test error: 0.49750\n",
      "[ 119]  Training error: 0.29812 Test error: 0.49750\n",
      "[ 120]  Training error: 0.29781 Test error: 0.49750\n",
      "[ 121]  Training error: 0.29719 Test error: 0.50000\n",
      "[ 122]  Training error: 0.29688 Test error: 0.50000\n",
      "[ 123]  Training error: 0.29688 Test error: 0.50500\n",
      "[ 124]  Training error: 0.29656 Test error: 0.50750\n",
      "[ 125]  Training error: 0.29594 Test error: 0.50750\n",
      "[ 126]  Training error: 0.29531 Test error: 0.50750\n",
      "[ 127]  Training error: 0.29375 Test error: 0.51000\n",
      "[ 128]  Training error: 0.29219 Test error: 0.51000\n",
      "[ 129]  Training error: 0.29125 Test error: 0.51000\n",
      "[ 130]  Training error: 0.29187 Test error: 0.51250\n",
      "[ 131]  Training error: 0.29125 Test error: 0.51250\n",
      "[ 132]  Training error: 0.29000 Test error: 0.51250\n",
      "[ 133]  Training error: 0.28719 Test error: 0.51250\n",
      "[ 134]  Training error: 0.28563 Test error: 0.51500\n",
      "[ 135]  Training error: 0.28531 Test error: 0.51250\n",
      "[ 136]  Training error: 0.28531 Test error: 0.52000\n",
      "[ 137]  Training error: 0.28344 Test error: 0.51750\n",
      "[ 138]  Training error: 0.28219 Test error: 0.51750\n",
      "[ 139]  Training error: 0.28094 Test error: 0.51750\n",
      "[ 140]  Training error: 0.27969 Test error: 0.51750\n",
      "[ 141]  Training error: 0.27875 Test error: 0.51750\n",
      "[ 142]  Training error: 0.27719 Test error: 0.51750\n",
      "[ 143]  Training error: 0.27656 Test error: 0.51750\n",
      "[ 144]  Training error: 0.27594 Test error: 0.51750\n",
      "[ 145]  Training error: 0.27406 Test error: 0.51750\n",
      "[ 146]  Training error: 0.27250 Test error: 0.51750\n",
      "[ 147]  Training error: 0.27094 Test error: 0.52000\n",
      "[ 148]  Training error: 0.27094 Test error: 0.52250\n",
      "[ 149]  Training error: 0.27063 Test error: 0.52750\n",
      "[ 150]  Training error: 0.26969 Test error: 0.52750\n",
      "[ 151]  Training error: 0.26750 Test error: 0.52750\n",
      "[ 152]  Training error: 0.26719 Test error: 0.52500\n",
      "[ 153]  Training error: 0.26656 Test error: 0.52750\n",
      "[ 154]  Training error: 0.26594 Test error: 0.52750\n",
      "[ 155]  Training error: 0.26469 Test error: 0.52500\n",
      "[ 156]  Training error: 0.26406 Test error: 0.52500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 157]  Training error: 0.26281 Test error: 0.52750\n",
      "[ 158]  Training error: 0.26219 Test error: 0.52750\n",
      "[ 159]  Training error: 0.26125 Test error: 0.52750\n",
      "[ 160]  Training error: 0.26094 Test error: 0.52500\n",
      "[ 161]  Training error: 0.26062 Test error: 0.52500\n",
      "[ 162]  Training error: 0.25938 Test error: 0.52500\n",
      "[ 163]  Training error: 0.25812 Test error: 0.52500\n",
      "[ 164]  Training error: 0.25625 Test error: 0.52500\n",
      "[ 165]  Training error: 0.25469 Test error: 0.53000\n",
      "[ 166]  Training error: 0.25438 Test error: 0.53250\n",
      "[ 167]  Training error: 0.25438 Test error: 0.53250\n",
      "[ 168]  Training error: 0.25344 Test error: 0.53750\n",
      "[ 169]  Training error: 0.25281 Test error: 0.53750\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MultiLayerPerceptron(layer_config=[140,20, len(input_genres)], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eta=0.001,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
