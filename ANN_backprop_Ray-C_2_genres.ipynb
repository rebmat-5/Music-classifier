{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "\n",
    "#def f_relu(X):\n",
    " #   return np.maximum(0,X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print(sys.stderr, err_str)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100,activation_o = f_softmax):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                        activation=f_sigmoid))\n",
    "\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=activation_o))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #Check!\n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        \n",
    "        #The differnce between the predicted labels from the neural network and the correct labels\n",
    "        #Calculates the derivate of the cost function\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        \n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "           #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            #Backpropagates the delta with respect to the derivate of the activation function\n",
    "            #Bakcpropagates the partial derivates\n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            #Delta is used to know how the weights should be adjusted to minimize the error\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=170, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                \n",
    "                #It updates the weighs by multiplying the error with the output from itself and the delta \n",
    "                #from the next node in the network\n",
    "                #eta is how aggressive the network \"corrects itself\" to changes\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    print((labels.shape[0], nbits))\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    print(bit_vector)\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hip-Hop': 0, 'Instrumental': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=['Hip-Hop','Instrumental']\n",
    "\n",
    "mydict={}\n",
    "i = 0\n",
    "for item in s:\n",
    "    if(i>0 and item in mydict):\n",
    "        continue\n",
    "    else:    \n",
    "       mydict[item] = i\n",
    "       i = i+1\n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_to_bit_vector(genres, nbits):\n",
    "        \n",
    "    #print((genres.shape[0], nbits))\n",
    "    bit_vector = np.zeros((genres.shape[0], nbits))\n",
    "    \n",
    "    #print(bit_vector)\n",
    "    nr_genres = 0\n",
    "    i =  0\n",
    "    while nr_genres<genres.shape[0]:\n",
    "        try:\n",
    "            #print(str(nr_genres) + \" \" + str(genres[i]) )\n",
    "            if(mydict[genres[i]]!= 0 | mydict[genres[i]]!= 4):\n",
    "                bit_vector[nr_genres, 2] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "            else:\n",
    "                #print(mydict[genres[i]])\n",
    "                bit_vector[nr_genres, mydict[genres[i]]] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "        except KeyError:\n",
    "            i += 1\n",
    "    \n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tracks = utils.load('./fma_metadata/tracks.csv')\n",
    "genres = utils.load('./fma_metadata/genres.csv')\n",
    "features = utils.load('./fma_metadata/features.csv')\n",
    "echonest = utils.load('./fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram(songID,resample=False ,sampling=44100, plot = True):\n",
    "    filepath = utils.get_audio_path('./fma_small/', songID)\n",
    "    try:\n",
    "        x, sr = librosa.load(filepath, sr=None, mono=True)  # kaiser_fast\n",
    "        print(sr)\n",
    "    except FileNotFoundError:\n",
    "        return -1\n",
    "        \n",
    "    \n",
    "    if(resample):\n",
    "        x = librosa.resample(x, sr, sampling)\n",
    "        sr = sampling\n",
    "    spec = librosa.feature.melspectrogram(y=x, sr=sr)\n",
    "    \n",
    "    if(plot):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis='mel', fmax=sr, x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel spectrogram')\n",
    "        plt.tight_layout()\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "#Xtr = Xtr.reshape(60000, 784)\n",
    "#X_test = X_test.reshape(10000, 784)\n",
    "#Xtr = Xtr.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#Xtr /= 255\n",
    "#X_test /= 255\n",
    "#print(Xtr.shape[0], 'train samples')\n",
    "#print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = tracks['set', 'subset'] <= 'small'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "val = tracks['set', 'split'] == 'validation'\n",
    "test = tracks['set', 'split'] == 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = tracks.loc[small, ('track', 'genre_top')]\n",
    "# Take out track_id for specified genres to create smaller subset\n",
    "mini_set = genres[(genres == 'Instrumental') | (genres == 'Hip-Hop')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = features.loc[small & test, 'mfcc']\n",
    "#X_test=X_test[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.values\n",
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = features.loc[small & train, 'mfcc']\n",
    "#Xtr =Xtr[:3200]\n",
    "Xtr = Xtr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(Xtr, axis=0)\n",
    "std = np.std(Xtr, axis=0)\n",
    "\n",
    "Xtr = (Xtr - mean)/std\n",
    "X_test = (X_test - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis = Xtr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this = spectogram(2,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ltr = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "#Ltr = Ltr[:3200] \n",
    "\n",
    "# Take out track_id for specified genres to create smaller subset\n",
    "Ltr_mini = Ltr[(genres == 'Instrumental') | (genres == 'Hip-Hop')]\n",
    "Xtr_mini = []\n",
    "i = 0\n",
    "for j in Ltr:\n",
    "    if((j == 'Hip-Hop') | (j == 'Instrumental')):\n",
    "        Xtr_mini.append(Xtr[i])\n",
    "        i += 1 \n",
    "Xtr_mini = np.asarray(Xtr_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(Xtr_mini))\n",
    "#print(len(Ltr_mini))\n",
    "#print(Xtr_mini)\n",
    "#print(Ltr_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_test = tracks.loc[small & test, ('track', 'genre_top')]\n",
    "\n",
    "# Take out track_id for specified genres to create smaller subset\n",
    "L_test_mini = L_test[(genres == 'Instrumental') | (genres == 'Hip-Hop')]\n",
    "X_test_mini = []\n",
    "i = 0\n",
    "for j in L_test:\n",
    "    if((j == 'Hip-Hop') | (j == 'Instrumental')):\n",
    "        X_test_mini.append(X_test[i])\n",
    "        i += 1 \n",
    "X_test_mini = np.asarray(X_test_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(X_test_mini))\n",
    "#print(len(L_test_mini))\n",
    "#print(X_test_mini)\n",
    "#print(L_test_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train\n",
    "#y_train=y_train.values\n",
    "#y_train=y_train._array_()\n",
    "#type(y_train)\n",
    "#y_train.shape\n",
    "#y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape\n",
    "\n",
    "type(Xtr)\n",
    "#Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print(\"This is : \" + str(N))\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    #print(data[:10])\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = genre_to_bit_vector(labels[idx:idx+batch_size], 2)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default activation eta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing input layer with size 140.\n",
      "Initializing hidden layer with size 70.\n",
      "Initializing hidden layer with size 20.\n",
      "Initializing output layer with size 2.\n",
      "Done!\n",
      "Training for 170 epochs...\n",
      "[   0]  Training error: 0.50000 Test error: 0.50000\n",
      "[   1]  Training error: 0.50000 Test error: 0.50000\n",
      "[   2]  Training error: 0.50000 Test error: 0.50000\n",
      "[   3]  Training error: 0.50000 Test error: 0.50000\n",
      "[   4]  Training error: 0.50000 Test error: 0.50000\n",
      "[   5]  Training error: 0.50000 Test error: 0.50000\n",
      "[   6]  Training error: 0.50000 Test error: 0.50000\n",
      "[   7]  Training error: 0.50000 Test error: 0.50000\n",
      "[   8]  Training error: 0.50000 Test error: 0.50000\n",
      "[   9]  Training error: 0.50000 Test error: 0.50000\n",
      "[  10]  Training error: 0.50000 Test error: 0.50000\n",
      "[  11]  Training error: 0.50000 Test error: 0.50000\n",
      "[  12]  Training error: 0.50000 Test error: 0.50000\n",
      "[  13]  Training error: 0.50000 Test error: 0.50000\n",
      "[  14]  Training error: 0.50000 Test error: 0.50000\n",
      "[  15]  Training error: 0.50000 Test error: 0.50000\n",
      "[  16]  Training error: 0.50000 Test error: 0.50000\n",
      "[  17]  Training error: 0.50000 Test error: 0.50000\n",
      "[  18]  Training error: 0.50000 Test error: 0.50000\n",
      "[  19]  Training error: 0.50000 Test error: 0.50000\n",
      "[  20]  Training error: 0.50000 Test error: 0.50000\n",
      "[  21]  Training error: 0.50000 Test error: 0.50000\n",
      "[  22]  Training error: 0.50000 Test error: 0.50000\n",
      "[  23]  Training error: 0.50000 Test error: 0.50000\n",
      "[  24]  Training error: 0.50000 Test error: 0.50000\n",
      "[  25]  Training error: 0.50000 Test error: 0.50000\n",
      "[  26]  Training error: 0.50000 Test error: 0.50000\n",
      "[  27]  Training error: 0.50000 Test error: 0.50000\n",
      "[  28]  Training error: 0.50000 Test error: 0.50000\n",
      "[  29]  Training error: 0.50000 Test error: 0.50000\n",
      "[  30]  Training error: 0.50000 Test error: 0.50000\n",
      "[  31]  Training error: 0.50000 Test error: 0.50000\n",
      "[  32]  Training error: 0.50000 Test error: 0.50000\n",
      "[  33]  Training error: 0.50000 Test error: 0.50000\n",
      "[  34]  Training error: 0.50000 Test error: 0.50000\n",
      "[  35]  Training error: 0.50000 Test error: 0.50000\n",
      "[  36]  Training error: 0.50000 Test error: 0.50000\n",
      "[  37]  Training error: 0.50000 Test error: 0.50000\n",
      "[  38]  Training error: 0.50000 Test error: 0.50000\n",
      "[  39]  Training error: 0.50000 Test error: 0.50000\n",
      "[  40]  Training error: 0.50000 Test error: 0.50000\n",
      "[  41]  Training error: 0.50000 Test error: 0.50000\n",
      "[  42]  Training error: 0.50000 Test error: 0.50000\n",
      "[  43]  Training error: 0.50000 Test error: 0.50000\n",
      "[  44]  Training error: 0.50000 Test error: 0.50000\n",
      "[  45]  Training error: 0.50000 Test error: 0.50000\n",
      "[  46]  Training error: 0.50000 Test error: 0.50000\n",
      "[  47]  Training error: 0.50000 Test error: 0.50000\n",
      "[  48]  Training error: 0.50000 Test error: 0.50000\n",
      "[  49]  Training error: 0.50000 Test error: 0.50000\n",
      "[  50]  Training error: 0.50000 Test error: 0.50000\n",
      "[  51]  Training error: 0.50000 Test error: 0.50000\n",
      "[  52]  Training error: 0.50000 Test error: 0.50000\n",
      "[  53]  Training error: 0.50000 Test error: 0.50000\n",
      "[  54]  Training error: 0.50000 Test error: 0.50000\n",
      "[  55]  Training error: 0.50000 Test error: 0.50000\n",
      "[  56]  Training error: 0.50000 Test error: 0.50000\n",
      "[  57]  Training error: 0.50000 Test error: 0.50000\n",
      "[  58]  Training error: 0.50000 Test error: 0.50000\n",
      "[  59]  Training error: 0.50000 Test error: 0.50000\n",
      "[  60]  Training error: 0.50000 Test error: 0.50000\n",
      "[  61]  Training error: 0.50000 Test error: 0.50000\n",
      "[  62]  Training error: 0.50000 Test error: 0.50000\n",
      "[  63]  Training error: 0.50000 Test error: 0.50000\n",
      "[  64]  Training error: 0.50000 Test error: 0.50000\n",
      "[  65]  Training error: 0.50000 Test error: 0.50000\n",
      "[  66]  Training error: 0.50000 Test error: 0.50000\n",
      "[  67]  Training error: 0.50000 Test error: 0.50000\n",
      "[  68]  Training error: 0.50000 Test error: 0.50000\n",
      "[  69]  Training error: 0.50000 Test error: 0.50000\n",
      "[  70]  Training error: 0.50000 Test error: 0.50000\n",
      "[  71]  Training error: 0.50000 Test error: 0.50000\n",
      "[  72]  Training error: 0.50000 Test error: 0.50000\n",
      "[  73]  Training error: 0.50000 Test error: 0.50000\n",
      "[  74]  Training error: 0.49938 Test error: 0.50000\n",
      "[  75]  Training error: 0.49938 Test error: 0.50000\n",
      "[  76]  Training error: 0.49875 Test error: 0.50500\n",
      "[  77]  Training error: 0.49812 Test error: 0.50500\n",
      "[  78]  Training error: 0.49562 Test error: 0.50500\n",
      "[  79]  Training error: 0.49125 Test error: 0.50000\n",
      "[  80]  Training error: 0.48625 Test error: 0.50000\n",
      "[  81]  Training error: 0.48375 Test error: 0.50000\n",
      "[  82]  Training error: 0.48125 Test error: 0.49000\n",
      "[  83]  Training error: 0.47250 Test error: 0.50000\n",
      "[  84]  Training error: 0.46813 Test error: 0.49500\n",
      "[  85]  Training error: 0.45875 Test error: 0.49500\n",
      "[  86]  Training error: 0.45500 Test error: 0.48000\n",
      "[  87]  Training error: 0.45062 Test error: 0.47000\n",
      "[  88]  Training error: 0.44562 Test error: 0.47500\n",
      "[  89]  Training error: 0.44188 Test error: 0.47000\n",
      "[  90]  Training error: 0.44000 Test error: 0.47500\n",
      "[  91]  Training error: 0.43875 Test error: 0.47500\n",
      "[  92]  Training error: 0.43688 Test error: 0.47000\n",
      "[  93]  Training error: 0.43688 Test error: 0.46500\n",
      "[  94]  Training error: 0.43750 Test error: 0.47000\n",
      "[  95]  Training error: 0.43312 Test error: 0.47000\n",
      "[  96]  Training error: 0.43125 Test error: 0.46000\n",
      "[  97]  Training error: 0.43000 Test error: 0.46000\n",
      "[  98]  Training error: 0.42812 Test error: 0.45500\n",
      "[  99]  Training error: 0.42562 Test error: 0.45000\n",
      "[ 100]  Training error: 0.42000 Test error: 0.44500\n",
      "[ 101]  Training error: 0.41813 Test error: 0.44500\n",
      "[ 102]  Training error: 0.41750 Test error: 0.44500\n",
      "[ 103]  Training error: 0.41625 Test error: 0.44000\n",
      "[ 104]  Training error: 0.41937 Test error: 0.44500\n",
      "[ 105]  Training error: 0.42125 Test error: 0.44000\n",
      "[ 106]  Training error: 0.42188 Test error: 0.44500\n",
      "[ 107]  Training error: 0.42250 Test error: 0.44000\n",
      "[ 108]  Training error: 0.42250 Test error: 0.44000\n",
      "[ 109]  Training error: 0.42063 Test error: 0.44000\n",
      "[ 110]  Training error: 0.42063 Test error: 0.43500\n",
      "[ 111]  Training error: 0.42000 Test error: 0.43000\n",
      "[ 112]  Training error: 0.41750 Test error: 0.43500\n",
      "[ 113]  Training error: 0.41687 Test error: 0.44000\n",
      "[ 114]  Training error: 0.41750 Test error: 0.44000\n",
      "[ 115]  Training error: 0.42063 Test error: 0.44000\n",
      "[ 116]  Training error: 0.42063 Test error: 0.44000\n",
      "[ 117]  Training error: 0.42063 Test error: 0.43500\n",
      "[ 118]  Training error: 0.42125 Test error: 0.43500\n",
      "[ 119]  Training error: 0.42188 Test error: 0.43500\n",
      "[ 120]  Training error: 0.42250 Test error: 0.43500\n",
      "[ 121]  Training error: 0.42063 Test error: 0.43500\n",
      "[ 122]  Training error: 0.42188 Test error: 0.43000\n",
      "[ 123]  Training error: 0.42063 Test error: 0.43000\n",
      "[ 124]  Training error: 0.42000 Test error: 0.43000\n",
      "[ 125]  Training error: 0.42063 Test error: 0.43000\n",
      "[ 126]  Training error: 0.42188 Test error: 0.43500\n",
      "[ 127]  Training error: 0.42188 Test error: 0.43000\n",
      "[ 128]  Training error: 0.42063 Test error: 0.43000\n",
      "[ 129]  Training error: 0.41937 Test error: 0.43000\n",
      "[ 130]  Training error: 0.41813 Test error: 0.42500\n",
      "[ 131]  Training error: 0.41875 Test error: 0.42500\n",
      "[ 132]  Training error: 0.41813 Test error: 0.42500\n",
      "[ 133]  Training error: 0.41813 Test error: 0.42500\n",
      "[ 134]  Training error: 0.41875 Test error: 0.42500\n",
      "[ 135]  Training error: 0.41937 Test error: 0.42500\n",
      "[ 136]  Training error: 0.42063 Test error: 0.42500\n",
      "[ 137]  Training error: 0.42000 Test error: 0.42500\n",
      "[ 138]  Training error: 0.42063 Test error: 0.42500\n",
      "[ 139]  Training error: 0.42250 Test error: 0.42500\n",
      "[ 140]  Training error: 0.42188 Test error: 0.43000\n",
      "[ 141]  Training error: 0.42125 Test error: 0.43000\n",
      "[ 142]  Training error: 0.42125 Test error: 0.43000\n",
      "[ 143]  Training error: 0.42188 Test error: 0.44000\n",
      "[ 144]  Training error: 0.42125 Test error: 0.44000\n",
      "[ 145]  Training error: 0.42125 Test error: 0.43500\n",
      "[ 146]  Training error: 0.42125 Test error: 0.43000\n",
      "[ 147]  Training error: 0.42188 Test error: 0.43000\n",
      "[ 148]  Training error: 0.42188 Test error: 0.43000\n",
      "[ 149]  Training error: 0.42312 Test error: 0.43000\n",
      "[ 150]  Training error: 0.42375 Test error: 0.43000\n",
      "[ 151]  Training error: 0.42375 Test error: 0.43000\n",
      "[ 152]  Training error: 0.42312 Test error: 0.43000\n",
      "[ 153]  Training error: 0.42312 Test error: 0.43000\n",
      "[ 154]  Training error: 0.42312 Test error: 0.43000\n",
      "[ 155]  Training error: 0.42312 Test error: 0.43500\n",
      "[ 156]  Training error: 0.42312 Test error: 0.43500\n",
      "[ 157]  Training error: 0.42375 Test error: 0.43500\n",
      "[ 158]  Training error: 0.42312 Test error: 0.43500\n",
      "[ 159]  Training error: 0.42312 Test error: 0.43500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 160]  Training error: 0.42438 Test error: 0.43500\n",
      "[ 161]  Training error: 0.42312 Test error: 0.43500\n",
      "[ 162]  Training error: 0.42250 Test error: 0.43500\n",
      "[ 163]  Training error: 0.42250 Test error: 0.43500\n",
      "[ 164]  Training error: 0.42312 Test error: 0.43500\n",
      "[ 165]  Training error: 0.42438 Test error: 0.43500\n",
      "[ 166]  Training error: 0.42438 Test error: 0.43500\n",
      "[ 167]  Training error: 0.42375 Test error: 0.43500\n",
      "[ 168]  Training error: 0.42438 Test error: 0.44000\n",
      "[ 169]  Training error: 0.42438 Test error: 0.43000\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#batch_size=200;\n",
    "\n",
    "#train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr_mini, Ltr_mini, X_test_mini, L_test_mini)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[140, 70, 20, 2], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eta=0.005,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
