{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "import sklearn.utils, sklearn.preprocessing, sklearn.decomposition, sklearn.svm\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "\n",
    "#def f_relu(X):\n",
    " #   return np.maximum(0,X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print(sys.stderr, err_str)\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100,activation_o = f_softmax):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                        activation=f_sigmoid))\n",
    "\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=activation_o))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #Check!\n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        \n",
    "        #The differnce between the predicted labels from the neural network and the correct labels\n",
    "        #Calculates the derivate of the cost function\n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        \n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "           #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            \n",
    "            #Backpropagates the delta with respect to the derivate of the activation function\n",
    "            #Bakcpropagates the partial derivates\n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            #Delta is used to know how the weights should be adjusted to minimize the error\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=170, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                \n",
    "                #It updates the weighs by multiplying the error with the output from itself and the delta \n",
    "                #from the next node in the network\n",
    "                #eta is how aggressive the network \"corrects itself\" to changes\n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    print((labels.shape[0], nbits))\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    print(bit_vector)\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hip-Hop': 0,\n",
       " 'Rock': 1,\n",
       " 'Pop': 2,\n",
       " 'Folk': 3,\n",
       " 'Experimental': 4,\n",
       " 'International': 5,\n",
       " 'Electronic': 6,\n",
       " 'Instrumental': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=['Hip-Hop','Rock','Pop','Folk','Experimental','International','Electronic','Instrumental']\n",
    "\n",
    "mydict={}\n",
    "i = 0\n",
    "for item in s:\n",
    "    if(i>0 and item in mydict):\n",
    "        continue\n",
    "    else:    \n",
    "       mydict[item] = i\n",
    "       i = i+1\n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_to_bit_vector(genres, nbits):\n",
    "        \n",
    "    #print((genres.shape[0], nbits))\n",
    "    bit_vector = np.zeros((genres.shape[0], nbits))\n",
    "    \n",
    "    #print(bit_vector)\n",
    "    nr_genres = 0\n",
    "    i =  0\n",
    "    while nr_genres<genres.shape[0]:\n",
    "        try:\n",
    "            #print(str(nr_genres) + \" \" + str(genres[i]) )\n",
    "            if(mydict[genres[i]]!= 0 | mydict[genres[i]]!= 4):\n",
    "                bit_vector[nr_genres, 2] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "            else:\n",
    "                #print(mydict[genres[i]])\n",
    "                bit_vector[nr_genres, mydict[genres[i]]] = 1.0\n",
    "                i += 1\n",
    "                nr_genres += 1\n",
    "        except KeyError:\n",
    "            i += 1\n",
    "    \n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/anaconda3/envs/D7041E/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tracks = utils.load('./fma_metadata/tracks.csv')\n",
    "genres = utils.load('./fma_metadata/genres.csv')\n",
    "features = utils.load('./fma_metadata/features.csv')\n",
    "echonest = utils.load('./fma_metadata/echonest.csv')\n",
    "\n",
    "np.testing.assert_array_equal(features.index, tracks.index)\n",
    "assert echonest.index.isin(tracks.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectogram(songID,resample=False ,sampling=44100, plot = True):\n",
    "    filepath = utils.get_audio_path('./fma_small/', songID)\n",
    "    try:\n",
    "        x, sr = librosa.load(filepath, sr=None, mono=True)  # kaiser_fast\n",
    "        print(sr)\n",
    "    except FileNotFoundError:\n",
    "        return -1\n",
    "        \n",
    "    \n",
    "    if(resample):\n",
    "        x = librosa.resample(x, sr, sampling)\n",
    "        sr = sampling\n",
    "    spec = librosa.feature.melspectrogram(y=x, sr=sr)\n",
    "    \n",
    "    if(plot):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis='mel', fmax=sr, x_axis='time')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Mel spectrogram')\n",
    "        plt.tight_layout()\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "#Xtr = Xtr.reshape(60000, 784)\n",
    "#X_test = X_test.reshape(10000, 784)\n",
    "#Xtr = Xtr.astype('float32')\n",
    "#X_test = X_test.astype('float32')\n",
    "#Xtr /= 255\n",
    "#X_test /= 255\n",
    "#print(Xtr.shape[0], 'train samples')\n",
    "#print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = tracks['set', 'subset'] <= 'small'\n",
    "\n",
    "train = tracks['set', 'split'] == 'training'\n",
    "val = tracks['set', 'split'] == 'validation'\n",
    "test = tracks['set', 'split'] == 'test'\n",
    "\n",
    "#genres = tracks.loc[small , ('track', 'genre_top')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = features.loc[small & test, 'mfcc']\n",
    "#X_test=X_test[:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.values\n",
    "type(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = features.loc[small & train, 'mfcc']\n",
    "#Xtr =Xtr[:3200]\n",
    "Xtr = Xtr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(Xtr, axis=0)\n",
    "std = np.std(Xtr, axis=0)\n",
    "\n",
    "Xtr = (Xtr - mean)/std\n",
    "X_test = (X_test - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis = Xtr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this = spectogram(2,plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ltr = tracks.loc[small & train, ('track', 'genre_top')]\n",
    "#Ltr = Ltr[:3200] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_test = tracks.loc[small & test, ('track', 'genre_top')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train\n",
    "#y_train=y_train.values\n",
    "#y_train=y_train._array_()\n",
    "#type(y_train)\n",
    "#y_train.shape\n",
    "#y_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape\n",
    "\n",
    "type(Xtr)\n",
    "#Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print(\"This is : \" + str(N))\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    #print(data[:10])\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = genre_to_bit_vector(labels[idx:idx+batch_size], 8)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default activation eta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "This is : 6400\n",
      "Batch size 800, the number of examples 6400.\n",
      "This is : 800\n",
      "Batch size 800, the number of examples 800.\n",
      "Done!\n",
      "Initializing input layer with size 140.\n",
      "Initializing hidden layer with size 70.\n",
      "Initializing hidden layer with size 20.\n",
      "Initializing output layer with size 8.\n",
      "Done!\n",
      "Training for 170 epochs...\n",
      "[   0]  Training error: 0.87531 Test error: 0.87625\n",
      "[   1]  Training error: 0.84437 Test error: 0.82625\n",
      "[   2]  Training error: 0.82375 Test error: 0.80375\n",
      "[   3]  Training error: 0.81625 Test error: 0.80500\n",
      "[   4]  Training error: 0.81094 Test error: 0.80375\n",
      "[   5]  Training error: 0.80656 Test error: 0.80500\n",
      "[   6]  Training error: 0.80422 Test error: 0.80750\n",
      "[   7]  Training error: 0.80328 Test error: 0.80625\n",
      "[   8]  Training error: 0.80266 Test error: 0.80875\n",
      "[   9]  Training error: 0.80188 Test error: 0.81125\n",
      "[  10]  Training error: 0.80125 Test error: 0.81125\n",
      "[  11]  Training error: 0.80156 Test error: 0.81125\n",
      "[  12]  Training error: 0.80078 Test error: 0.81125\n",
      "[  13]  Training error: 0.80156 Test error: 0.81000\n",
      "[  14]  Training error: 0.80437 Test error: 0.81125\n",
      "[  15]  Training error: 0.80469 Test error: 0.81000\n",
      "[  16]  Training error: 0.80312 Test error: 0.81500\n",
      "[  17]  Training error: 0.79984 Test error: 0.81375\n",
      "[  18]  Training error: 0.79641 Test error: 0.81000\n",
      "[  19]  Training error: 0.79531 Test error: 0.80750\n",
      "[  20]  Training error: 0.79109 Test error: 0.80000\n",
      "[  21]  Training error: 0.78891 Test error: 0.79500\n",
      "[  22]  Training error: 0.78312 Test error: 0.78750\n",
      "[  23]  Training error: 0.77703 Test error: 0.77875\n",
      "[  24]  Training error: 0.77641 Test error: 0.77000\n",
      "[  25]  Training error: 0.77234 Test error: 0.76875\n",
      "[  26]  Training error: 0.76844 Test error: 0.76375\n",
      "[  27]  Training error: 0.76687 Test error: 0.76250\n",
      "[  28]  Training error: 0.76281 Test error: 0.76000\n",
      "[  29]  Training error: 0.76078 Test error: 0.76375\n",
      "[  30]  Training error: 0.75891 Test error: 0.77000\n",
      "[  31]  Training error: 0.75625 Test error: 0.76250\n",
      "[  32]  Training error: 0.75422 Test error: 0.76500\n",
      "[  33]  Training error: 0.75141 Test error: 0.76250\n",
      "[  34]  Training error: 0.75281 Test error: 0.76125\n",
      "[  35]  Training error: 0.75156 Test error: 0.76250\n",
      "[  36]  Training error: 0.74969 Test error: 0.76750\n",
      "[  37]  Training error: 0.74422 Test error: 0.76125\n",
      "[  38]  Training error: 0.71781 Test error: 0.71625\n",
      "[  39]  Training error: 0.68875 Test error: 0.68750\n",
      "[  40]  Training error: 0.66625 Test error: 0.69375\n",
      "[  41]  Training error: 0.65594 Test error: 0.69000\n",
      "[  42]  Training error: 0.65078 Test error: 0.68875\n",
      "[  43]  Training error: 0.64797 Test error: 0.68500\n",
      "[  44]  Training error: 0.64203 Test error: 0.68250\n",
      "[  45]  Training error: 0.63828 Test error: 0.68875\n",
      "[  46]  Training error: 0.63313 Test error: 0.68625\n",
      "[  47]  Training error: 0.62875 Test error: 0.68500\n",
      "[  48]  Training error: 0.62062 Test error: 0.69375\n",
      "[  49]  Training error: 0.61641 Test error: 0.68375\n",
      "[  50]  Training error: 0.60828 Test error: 0.67875\n",
      "[  51]  Training error: 0.59937 Test error: 0.65375\n",
      "[  52]  Training error: 0.59734 Test error: 0.64875\n",
      "[  53]  Training error: 0.59016 Test error: 0.63250\n",
      "[  54]  Training error: 0.58453 Test error: 0.62500\n",
      "[  55]  Training error: 0.57891 Test error: 0.62500\n",
      "[  56]  Training error: 0.57469 Test error: 0.62625\n",
      "[  57]  Training error: 0.56766 Test error: 0.62250\n",
      "[  58]  Training error: 0.56469 Test error: 0.62375\n",
      "[  59]  Training error: 0.56172 Test error: 0.62375\n",
      "[  60]  Training error: 0.55781 Test error: 0.62875\n",
      "[  61]  Training error: 0.55422 Test error: 0.62625\n",
      "[  62]  Training error: 0.55078 Test error: 0.62125\n",
      "[  63]  Training error: 0.54844 Test error: 0.61750\n",
      "[  64]  Training error: 0.54563 Test error: 0.62000\n",
      "[  65]  Training error: 0.54422 Test error: 0.62000\n",
      "[  66]  Training error: 0.54281 Test error: 0.61875\n",
      "[  67]  Training error: 0.54016 Test error: 0.62000\n",
      "[  68]  Training error: 0.53531 Test error: 0.61750\n",
      "[  69]  Training error: 0.52891 Test error: 0.61000\n",
      "[  70]  Training error: 0.52359 Test error: 0.60750\n",
      "[  71]  Training error: 0.51922 Test error: 0.60875\n",
      "[  72]  Training error: 0.51469 Test error: 0.61250\n",
      "[  73]  Training error: 0.51016 Test error: 0.61625\n",
      "[  74]  Training error: 0.50875 Test error: 0.61000\n",
      "[  75]  Training error: 0.50484 Test error: 0.59625\n",
      "[  76]  Training error: 0.49625 Test error: 0.59500\n",
      "[  77]  Training error: 0.49203 Test error: 0.59125\n",
      "[  78]  Training error: 0.49156 Test error: 0.60625\n",
      "[  79]  Training error: 0.50187 Test error: 0.61500\n",
      "[  80]  Training error: 0.48703 Test error: 0.61250\n",
      "[  81]  Training error: 0.48172 Test error: 0.59500\n",
      "[  82]  Training error: 0.47969 Test error: 0.58750\n",
      "[  83]  Training error: 0.48031 Test error: 0.60500\n",
      "[  84]  Training error: 0.48438 Test error: 0.61125\n",
      "[  85]  Training error: 0.48063 Test error: 0.60875\n",
      "[  86]  Training error: 0.47094 Test error: 0.60500\n",
      "[  87]  Training error: 0.46953 Test error: 0.59000\n",
      "[  88]  Training error: 0.46953 Test error: 0.59250\n",
      "[  89]  Training error: 0.47172 Test error: 0.60375\n",
      "[  90]  Training error: 0.47437 Test error: 0.60750\n",
      "[  91]  Training error: 0.46609 Test error: 0.60375\n",
      "[  92]  Training error: 0.45844 Test error: 0.59250\n",
      "[  93]  Training error: 0.45563 Test error: 0.58000\n",
      "[  94]  Training error: 0.45328 Test error: 0.58500\n",
      "[  95]  Training error: 0.46047 Test error: 0.60250\n",
      "[  96]  Training error: 0.46063 Test error: 0.60750\n",
      "[  97]  Training error: 0.45125 Test error: 0.59750\n",
      "[  98]  Training error: 0.44000 Test error: 0.58750\n",
      "[  99]  Training error: 0.43391 Test error: 0.58500\n",
      "[ 100]  Training error: 0.43406 Test error: 0.58250\n",
      "[ 101]  Training error: 0.44297 Test error: 0.59500\n",
      "[ 102]  Training error: 0.44906 Test error: 0.61125\n",
      "[ 103]  Training error: 0.43422 Test error: 0.59625\n",
      "[ 104]  Training error: 0.42484 Test error: 0.58125\n",
      "[ 105]  Training error: 0.41437 Test error: 0.58000\n",
      "[ 106]  Training error: 0.41219 Test error: 0.57625\n",
      "[ 107]  Training error: 0.41422 Test error: 0.57375\n",
      "[ 108]  Training error: 0.44125 Test error: 0.59500\n",
      "[ 109]  Training error: 0.43375 Test error: 0.60875\n",
      "[ 110]  Training error: 0.40891 Test error: 0.58250\n",
      "[ 111]  Training error: 0.40125 Test error: 0.58750\n",
      "[ 112]  Training error: 0.39859 Test error: 0.58375\n",
      "[ 113]  Training error: 0.40563 Test error: 0.58875\n",
      "[ 114]  Training error: 0.43391 Test error: 0.58500\n",
      "[ 115]  Training error: 0.42328 Test error: 0.59500\n",
      "[ 116]  Training error: 0.39469 Test error: 0.59375\n",
      "[ 117]  Training error: 0.38094 Test error: 0.58250\n",
      "[ 118]  Training error: 0.39250 Test error: 0.58500\n",
      "[ 119]  Training error: 0.41703 Test error: 0.58250\n",
      "[ 120]  Training error: 0.41719 Test error: 0.59750\n",
      "[ 121]  Training error: 0.38828 Test error: 0.59375\n",
      "[ 122]  Training error: 0.37422 Test error: 0.58500\n",
      "[ 123]  Training error: 0.37781 Test error: 0.58375\n",
      "[ 124]  Training error: 0.39875 Test error: 0.57875\n",
      "[ 125]  Training error: 0.39906 Test error: 0.59375\n",
      "[ 126]  Training error: 0.38016 Test error: 0.59250\n",
      "[ 127]  Training error: 0.36781 Test error: 0.58875\n",
      "[ 128]  Training error: 0.37188 Test error: 0.59125\n",
      "[ 129]  Training error: 0.37594 Test error: 0.58875\n",
      "[ 130]  Training error: 0.38875 Test error: 0.58375\n",
      "[ 131]  Training error: 0.37781 Test error: 0.59250\n",
      "[ 132]  Training error: 0.36141 Test error: 0.59000\n",
      "[ 133]  Training error: 0.36297 Test error: 0.59375\n",
      "[ 134]  Training error: 0.34781 Test error: 0.58500\n",
      "[ 135]  Training error: 0.36188 Test error: 0.58625\n",
      "[ 136]  Training error: 0.37328 Test error: 0.59250\n",
      "[ 137]  Training error: 0.36203 Test error: 0.60000\n",
      "[ 138]  Training error: 0.34672 Test error: 0.59375\n",
      "[ 139]  Training error: 0.33516 Test error: 0.58750\n",
      "[ 140]  Training error: 0.35734 Test error: 0.58750\n",
      "[ 141]  Training error: 0.36453 Test error: 0.58875\n",
      "[ 142]  Training error: 0.35187 Test error: 0.60625\n",
      "[ 143]  Training error: 0.34266 Test error: 0.59625\n",
      "[ 144]  Training error: 0.32641 Test error: 0.59500\n",
      "[ 145]  Training error: 0.34063 Test error: 0.59250\n",
      "[ 146]  Training error: 0.34437 Test error: 0.58750\n",
      "[ 147]  Training error: 0.34047 Test error: 0.60625\n",
      "[ 148]  Training error: 0.32844 Test error: 0.59500\n",
      "[ 149]  Training error: 0.31547 Test error: 0.59625\n",
      "[ 150]  Training error: 0.33953 Test error: 0.59875\n",
      "[ 151]  Training error: 0.34047 Test error: 0.59500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 152]  Training error: 0.32922 Test error: 0.60250\n",
      "[ 153]  Training error: 0.31453 Test error: 0.59625\n",
      "[ 154]  Training error: 0.31031 Test error: 0.60500\n",
      "[ 155]  Training error: 0.32891 Test error: 0.59625\n",
      "[ 156]  Training error: 0.32672 Test error: 0.59625\n",
      "[ 157]  Training error: 0.31734 Test error: 0.59625\n",
      "[ 158]  Training error: 0.29609 Test error: 0.60375\n",
      "[ 159]  Training error: 0.29219 Test error: 0.60375\n",
      "[ 160]  Training error: 0.32297 Test error: 0.59625\n",
      "[ 161]  Training error: 0.32109 Test error: 0.59750\n",
      "[ 162]  Training error: 0.31125 Test error: 0.60375\n",
      "[ 163]  Training error: 0.29500 Test error: 0.59750\n",
      "[ 164]  Training error: 0.28828 Test error: 0.60875\n",
      "[ 165]  Training error: 0.30109 Test error: 0.59500\n",
      "[ 166]  Training error: 0.30109 Test error: 0.60750\n",
      "[ 167]  Training error: 0.30453 Test error: 0.60375\n",
      "[ 168]  Training error: 0.28219 Test error: 0.60750\n",
      "[ 169]  Training error: 0.27047 Test error: 0.61875\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=800;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[140, 70, 20, 8], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,eta=0.005,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
